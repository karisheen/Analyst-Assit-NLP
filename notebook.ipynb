{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8a22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b3175",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'comp.security' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m CATEGORIES_RAW = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcomp.security\u001b[39m\u001b[33m\"\u001b[39m,          \u001b[38;5;66;03m# Cyber\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtalk.politics.misc\u001b[39m\u001b[33m\"\u001b[39m,     \u001b[38;5;66;03m# Compliance-ish (proxy)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmisc.forsale\u001b[39m\u001b[33m\"\u001b[39m,           \u001b[38;5;66;03m# Fraud/other proxy\u001b[39;00m\n\u001b[32m      8\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m raw = \u001b[43mfetch_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCATEGORIES_RAW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheaders\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfooters\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquotes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Map raw newsgroups to your demo labels\u001b[39;00m\n\u001b[32m     13\u001b[39m MAP_TO_LABEL = {\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcomp.security\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mCyber\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtalk.politics.misc\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mSanctions/Compliance\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmisc.forsale\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mFraud\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal-Projects/Analyst-Assit-NLP/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal-Projects/Analyst-Assit-NLP/.venv/lib/python3.13/site-packages/sklearn/datasets/_twenty_newsgroups.py:357\u001b[39m, in \u001b[36mfetch_20newsgroups\u001b[39m\u001b[34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y, n_retries, delay)\u001b[39m\n\u001b[32m    354\u001b[39m     data.data = [strip_newsgroup_quoting(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m data.data]\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m categories \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     labels = [(\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m)\u001b[49m, cat) \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m categories]\n\u001b[32m    358\u001b[39m     \u001b[38;5;66;03m# Sort the categories to have the ordering of the labels\u001b[39;00m\n\u001b[32m    359\u001b[39m     labels.sort()\n",
      "\u001b[31mValueError\u001b[39m: 'comp.security' is not in list"
     ]
    }
   ],
   "source": [
    "CATEGORIES_RAW = [\n",
    "    \"sci.crypt\",              # Cyber\n",
    "    \"talk.politics.misc\",     # Compliance-ish (proxy)\n",
    "    \"talk.politics.guns\",     # Physical threat proxy\n",
    "    \"soc.religion.christian\", # Harassment/other proxy\n",
    "    \"rec.autos\",              # Fraud/other proxy\n",
    "    \"misc.forsale\",           # Fraud/other proxy\n",
    "]\n",
    "\n",
    "raw = fetch_20newsgroups(subset=\"all\", categories=CATEGORIES_RAW, remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "\n",
    "# Map raw newsgroups to your demo labels\n",
    "MAP_TO_LABEL = {\n",
    "    \"sci.crypt\": \"Cyber\",\n",
    "    \"talk.politics.misc\": \"Sanctions/Compliance\",\n",
    "    \"talk.politics.guns\": \"Physical Threat\",\n",
    "    \"soc.religion.christian\": \"Harassment\",\n",
    "    \"rec.autos\": \"Other\",\n",
    "    \"misc.forsale\": \"Fraud\",\n",
    "}\n",
    "\n",
    "labels = [MAP_TO_LABEL[raw.target_names[t]] for t in raw.target]\n",
    "\n",
    "df = pd.DataFrame({\"text\": raw.data, \"label\": labels})\n",
    "df = df[df[\"text\"].str.len() > 50].reset_index(drop=True)  # basic cleanup\n",
    "df[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\-\\']\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"clean\"] = df[\"text\"].astype(str).map(clean_text)\n",
    "df[[\"label\", \"clean\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fed835",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"clean\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test), pd.Series(y_train).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=3,\n",
    "        max_df=0.9,\n",
    "        stop_words=\"english\"\n",
    "    )),\n",
    "    (\"model\", LogisticRegression(max_iter=2000, n_jobs=None))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sorted = sorted(df[\"label\"].unique())\n",
    "cm = confusion_matrix(y_test, pred, labels=labels_sorted)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.xticks(range(len(labels_sorted)), labels_sorted, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(labels_sorted)), labels_sorted)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8775a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = clf.named_steps[\"tfidf\"]\n",
    "model = clf.named_steps[\"model\"]\n",
    "\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "classes = model.classes_\n",
    "\n",
    "top_k = 12\n",
    "rows = []\n",
    "for i, cls in enumerate(classes):\n",
    "    coefs = model.coef_[i]\n",
    "    top_pos_idx = np.argsort(coefs)[-top_k:][::-1]\n",
    "    rows.append((cls, \", \".join(feature_names[top_pos_idx])))\n",
    "\n",
    "pd.DataFrame(rows, columns=[\"class\", \"top_terms\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEVERITY = {\n",
    "    \"Physical Threat\": 1.00,\n",
    "    \"Cyber\": 0.85,\n",
    "    \"Sanctions/Compliance\": 0.75,\n",
    "    \"Fraud\": 0.70,\n",
    "    \"Harassment\": 0.55,\n",
    "    \"Other\": 0.30\n",
    "}\n",
    "\n",
    "proba = clf.predict_proba(X_test)\n",
    "pred_labels = clf.predict(X_test)\n",
    "\n",
    "# risk score = max prob * severity(label) * 100\n",
    "max_proba = proba.max(axis=1)\n",
    "severity = np.array([SEVERITY[l] for l in pred_labels])\n",
    "risk_score = (max_proba * severity * 100).round(1)\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"text\": df.loc[X_test.index, \"text\"].values,\n",
    "    \"clean\": X_test.values,\n",
    "    \"predicted_category\": pred_labels,\n",
    "    \"confidence\": max_proba.round(3),\n",
    "    \"risk_score\": risk_score\n",
    "})\n",
    "\n",
    "out.sort_values(\"risk_score\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_keywords_for_doc(text, vectorizer, top_n=8):\n",
    "    vec = vectorizer.transform([text])\n",
    "    if vec.nnz == 0:\n",
    "        return []\n",
    "    idx = np.argsort(vec.data)[-top_n:][::-1]\n",
    "    feature_idx = vec.indices[idx]\n",
    "    return feature_names[feature_idx].tolist()\n",
    "\n",
    "def analyst_summary(row):\n",
    "    kws = top_keywords_for_doc(row[\"clean\"], vectorizer, top_n=6)\n",
    "    return f\"{row['predicted_category']} signal detected. Key terms: {', '.join(kws[:6])}.\"\n",
    "\n",
    "out[\"top_keywords\"] = out[\"clean\"].apply(lambda s: top_keywords_for_doc(s, vectorizer, top_n=8))\n",
    "out[\"analyst_summary\"] = out.apply(analyst_summary, axis=1)\n",
    "\n",
    "out[[\"predicted_category\", \"risk_score\", \"confidence\", \"top_keywords\", \"analyst_summary\"]].head(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c63c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text, max_ents=8):\n",
    "    doc = nlp(text[:2000])  # cap length for speed\n",
    "    ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\", \"LOC\", \"DATE\"}:\n",
    "            ents.append(f\"{ent.text} ({ent.label_})\")\n",
    "    # de-dup while preserving order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for e in ents:\n",
    "        if e not in seen:\n",
    "            uniq.append(e)\n",
    "            seen.add(e)\n",
    "    return uniq[:max_ents]\n",
    "\n",
    "out[\"key_entities\"] = out[\"text\"].apply(extract_entities)\n",
    "out[[\"predicted_category\", \"risk_score\", \"key_entities\", \"analyst_summary\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2776056",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_view = out.copy()\n",
    "analyst_view[\"text\"] = analyst_view[\"text\"].str.replace(r\"\\s+\", \" \", regex=True).str.slice(0, 260) + \"â€¦\"\n",
    "\n",
    "analyst_view = analyst_view[[\n",
    "    \"text\",\n",
    "    \"predicted_category\",\n",
    "    \"risk_score\",\n",
    "    \"confidence\",\n",
    "    \"top_keywords\",\n",
    "    \"key_entities\",\n",
    "    \"analyst_summary\"\n",
    "]].sort_values(\"risk_score\", ascending=False)\n",
    "\n",
    "analyst_view.head(15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
